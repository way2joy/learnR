분류 알고리즘
=====================================================

## 1. 데이터 탐색

### 1.1 기술통계

```{r}
library(doBy)
summary(iris)

library(Hmisc)
describe(iris)
```

```{r}
plot(iris)
plot(Species~Sepal.Length, data=iris)
plot(iris$Sepal.Length, col=as.numeric(iris$Species))
```

```{r}
library(caret)
featurePlot(iris[,1:4], iris$Species, "ellipse")
```

## 2. 전처리

### 2.1 데이터 변환

#### 데이터 정규화(Feature Scaling)

`scale()` : 행렬과 데이터 프레임의 경우엔 각 열을 독립적으로 정규화시킨 뒤 정규화된 값(z점수)으로 이루어진 
열들을 행렬로 반환한다. 

(정규화하고 싶은 값-평균)/표준편차 = z점수(정규화된 값) 

+ `center` : 값에서 평균을 뺄지 여부
+ `scale` : 값을 표준편차로 나눌지 여부

```{r}
tmp.scaled <- scale(iris[,-5])
iris.scaled <- cbind(as.data.frame(tmp.scaled), Species=iris$Species)
featurePlot(iris.scaled[,-5], iris.scaled$Species, "ellipse")
```

#### 주성분분석 PCA

PCA(Principal Componenet Analysis) : 차원감소(Dimensionality Reduction) 또는 데이터를 독립차원으로 재표현하기 위하여 사용하는 방법

`princomp()`

`predict()`

```{r}
iris.p <- princomp(iris[,-5], cor=T)
summary(iris.p)
plot(iris.p, type='l')

iris.pd <- predict(iris.p, iris[,-5])
plot(iris.pd)
```

#### 범주형 변수의 재표현

One Hot Encoding : 여러개의 가변수(dummy variables)를 사용해 범주형 변수를 재표현 하는 것

`model.matrix()`

```{r}
x <- data.frame(lvl=factor(c("A", "B", "A", "A", "C")), value=c(1,3,2,4,5))

model.matrix(~lvl, data=x)[,-1]
```

### 2.2 결측값(NA)의 처리

결측값 찾기 : `complete.cases()`

```{r}
iris.na <- iris
iris.na[c(10,20,25,40,32), 3] <- NA
iris.na[c(33,100,123), 1] <- NA
iris.na[!complete.cases(iris.na),]  # 전체 열 가운데 하나라도 NA가 있으면

iris.na[is.na(iris.na$Sepal.Length),] # 하나의 열에 대해서만 
```

NA를 중앙값으로 대치한 예
`centralImputation()`
```{r}
library(DMwR)
na.location <- !complete.cases(iris.na)
iris.na.md <- centralImputation(iris.na)
# 확인
iris.na.md[na.location,-5]
```

kNN을 사용하여 k개 근접 이웃의 가중 평균으로 대치한 예
가중치는 NA값이 있는 데이터와의 거리로부터 계산됨
`knnImputation()` : scale() 자동 실행

```{r}
iris.na.knn <- knnImputation(iris.na)
# 확인
iris.na.knn[na.location,-5]
```

### 2.3 변수 선택(Feature Selection)

주어진 데이터의 전부를 사용하기보다는 그 중 모델링에 가장 적합한 변수만 택하는 과정

+ Filter Method : 통계적 특성 기반
+ Wrapper Method : 모델링 반복 수행
+ Embeded method : 

#### 0에 가까운 분산(Near Zero Variance)

변수 선택의 가장 단순한 방법은 변수값의 분산을 보는 것. 

분산이 작은 변수는 모델에 큰 영향을 주지 않는다. 

`nearZeroVar(){caret}`

```{r}
library(caret)
library(mlbench) ; data(Soybean)
str(Soybean)

nearZeroVar(Soybean, saveMetrics=T)     # saveMetrics=T : 분석결과 출력
# 결과의 "nzv"는 0에 가까운 분산을 의미함. 따라서 이 값이 TRUE인 변수들을 제거할 수 있다.

Soybean.rm.nzv <- Soybean[,-nearZeroVar(Soybean)]
str(Soybean.rm.nzv)
```


